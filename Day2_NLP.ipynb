{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2 NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbknjpakXmKVGX+MIV6mde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaranyaRavikumar06/Itrain-Advance-Python/blob/main/Day2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n285p2iBwPnV"
      },
      "outputs": [],
      "source": [
        "text = \"\"\" Dostoevsky,was the son of the doctorâ€¦...\"\"\"\n",
        "#List The vocabulary\n",
        "vocab = sorted(set(text.lower().split()))\n",
        "vocab\n",
        "#Count the vocabulary\n",
        "len(vocab)\n",
        "#Count the occuranece\n",
        "text.count('the')\n",
        "#Filter Words\n",
        "text.split(\".\")\n",
        "text.split()\n",
        "\n",
        "stop_word=[\".\",\",\",\"a\",\"the\"]\n",
        "a=[word for word in text if word not in stop_word]\n",
        "a\n",
        "\n",
        "\n",
        "#Example\n",
        "\n",
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "#Encode the data \n",
        "encoding = response.info().get_param('charset', 'utf8')\n",
        "text1 = response.read().decode(encoding)\n",
        "text1\n",
        "# Count total no of words\n",
        "vocab1 = sorted(set(text1.lower().split()))\n",
        "len(vocab1)\n",
        "longword=[word for word in vocab1 if len(word)>5]\n",
        "longword\n",
        "\n",
        "# Tokenize special words\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "print(len(sent_tokenize(text1)))\n",
        "print(len(word_tokenize(text1)))\n",
        "\n",
        "\n",
        "#Tokenzing\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
        "\n",
        "print(sent_tokenize(EXAMPLE_TEXT))\n",
        "print(word_tokenize(EXAMPLE_TEXT))\n",
        "\n",
        "#Or the word tokenize can be usedlike this\n",
        "for i in word_tokenize(EXAMPLE_TEXT):\n",
        "    print(i)\n",
        "\n",
        "\n",
        "#nltk import\n",
        "#pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "\n",
        "#STOP WORDS\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#list the default stopwords\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "#Example\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "print(word_tokens)\n",
        "#Create a filtered sentence\n",
        "filtered_sentence=[]\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "# or the above code can be replaced as\n",
        "#   filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "print(filtered_sentence)\n",
        "\n",
        "\n",
        "# Remove Punctuation from text\\\n",
        "from string import punctuation\n",
        "def strip_punctuation(s):\n",
        "    return ''.join(c for c in s if c not in punctuation)\n",
        "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "print (strip_punctuation(text))\n",
        "   \n",
        "# Remove numbers from text\n",
        "text = \"There was 200 people standing right next to me at 2pm.\"\n",
        "output = ''.join(c for c in text if not c.isdigit())\n",
        "print(output)\n",
        "\n",
        "\n",
        "#Remove Html tags in a text\n",
        "import re\n",
        "text = \"\"\"<head><body>hello world!</body></head>\"\"\"\n",
        "cleaned_text = re.sub('<[^<]+?>','', text)\n",
        "print (cleaned_text)\n",
        "\n",
        "#Regular Expression for detecting Word Patterns\n",
        "import re\n",
        "import nltk\n",
        "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]#List the words in the English Dictionary words\n",
        "a=[w for w in wordlist if re.search('ed$', w)]#List the words in the wordlist ending with ed\n",
        "len(a)\n",
        "#The . wildcard symbol matches any single character. Suppose we have room in a crossword puzzle for an 8-letter word with j as its third letter and t as its sixth letter. In place of each blank cell we use a period:\n",
        "[w for w in wordlist if re.search('^..j..t..$', w)]\n",
        "[w for w in wordlist if re.match('^[a-z]+$',w)]\n",
        "\n",
        "\n",
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))\n",
        "    \n",
        "#Example stemming\n",
        "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
        "words=word_tokenize(new_text)\n",
        "for w in words:\n",
        "    print(ps.stem(w))\n",
        "\n",
        "\n",
        "#Lemmatize\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos=\"v\")))\n",
        "\n",
        "\n",
        "\n",
        "#POS\n",
        "nltk.pos_tag(['cat','cats'])\n",
        "nltk.pos_tag(['take','took','taking','taken'])\n",
        "nltk.pos_tag(['delicious'])\n",
        "nltk.pos_tag(['slowly'])\n",
        "text = word_tokenize(\"And now for something completely different\")\n",
        "nltk.pos_tag(text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#EXAMPLE\n",
        "from urllib import request\n",
        "url = \"https://en.wikipedia.org/wiki/George_Washington\"\n",
        "response = request.urlopen(url)\n",
        "#Encode the data \n",
        "encoding = response.info().get_param('charset', 'utf8')\n",
        "text1 = response.read().decode(encoding)\n",
        "text1\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "print(len(sent_tokenize(text1)))\n",
        "print(len(word_tokenize(text1)))\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text2=strip_punctuation(text1)\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "word_tokens = nltk.word_tokenize(text2)\n",
        "word_tokens1=''.join(c for c in word_tokens if not c.isdigit())\n",
        "print(word_tokens1)\n",
        "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "print (lemmatized_word)\n",
        "nltk.pos_tag(text2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#brown\n",
        "#In the rest of this chapter we will explore various ways to automatically add part-of-speech tags to text. We will see that the tag of a word depends on the word and its context within a sentence. For this reason, we will be working with data at the level of (tagged) sentences rather than words. We'll begin by loading the data we will be using.\n",
        "from nltk.corpus import brown\n",
        "brown_sents = brown.sents(categories='news')\n",
        "brown_sents\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "brown_tagged_sents\n",
        "\n",
        "\n",
        "\n",
        "#MODULE 3\n",
        "\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "fileid = 'austen-emma.txt'\n",
        "text = gutenberg.raw(fileid)\n",
        "#Fileid : \n",
        "gutenberg.fileids()\n",
        "#Text : \n",
        "gutenberg.raw(fileid)\n",
        "#Words : \n",
        "gutenberg.words(fileid)\n",
        "#Sentence : \n",
        "gutenberg.sents(fileid)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "tok = sent_tokenize(text)\n",
        "\n",
        "for x in range(5):\n",
        "    print(tok[x])\n",
        "    \n",
        "from nltk.corpus import brown\n",
        "brown.categories()\n",
        "from nltk.corpus import brown\n",
        "brown.categories()\n",
        "text = brown.raw(categories='news')\n",
        "\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "reuters.fileids()\n",
        "reuters.categories()\n",
        "fileid='test/16399'\n",
        "text = reuters.raw(fileid)\n",
        "text1=reuters.raw(categories='zinc')\n",
        "\n",
        "\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "movie_reviews.fileids()\n",
        "movie_reviews.categories()\n",
        "fileid=''\n",
        "text = movie_reviews.raw(fileid)\n",
        "text1=movie_reviews.raw(categories='')\n",
        "\n",
        "\n",
        "#Frequency distribution by creating our own corpus\n",
        "\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "fileid = 'C:/Users/arun/Desktop/ITRAIN/itrain python/Advanced/codes/DAY 2/gaming.txt'\n",
        "my_corpus = PlaintextCorpusReader(fileid, '.*')\n",
        "text = my_corpus.raw(fileid)\n",
        "text\n",
        "my_corpus.fileids()\n",
        "my_corpus.raw(fileid)\n",
        "my_corpus.words(fileid)\n",
        "my_corpus.sents(fileid)\n",
        "distr = nltk.FreqDist(text)\n",
        "print(distr.most_common(5))\n",
        "\n",
        "\n",
        "\n",
        "#Reuters\n",
        "from nltk.corpus import reuters\n",
        "fileid='training/9865'\n",
        "text=reuters.raw(fileid)\n",
        "text"
      ]
    }
  ]
}